% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpu_multiply.R
\name{gpu_dot}
\alias{gpu_dot}
\title{GPU-accelerated Dot Product}
\usage{
gpu_dot(a, b, force_cpu = FALSE, warn_fallback = TRUE)
}
\arguments{
\item{a}{A numeric vector}

\item{b}{A numeric vector of the same length as \code{a}}

\item{force_cpu}{Logical. If TRUE, forces CPU implementation (for testing/fallback)}

\item{warn_fallback}{Logical. If TRUE, warns when falling back to CPU implementation}
}
\value{
A numeric scalar containing the dot product of \code{a} and \code{b}
}
\description{
Computes the dot product of two numeric vectors using CUDA on the GPU.
This function provides a GPU-accelerated alternative for computing the
inner product of two vectors.
}
\details{
This function transfers the input vectors to GPU memory, performs the dot
product using a CUDA kernel with parallel reduction, and returns the scalar
result. The reduction operation uses shared memory for efficient computation.

If GPU is not available or GPU operations fail, the function automatically
falls back to CPU computation with an optional warning.
}
\examples{
\dontrun{
# Compute dot product of large vectors on GPU
n <- 1e6
a <- runif(n)
b <- runif(n)
result <- gpu_dot(a, b)

# Verify correctness against CPU
all.equal(result, sum(a * b))
}

}
