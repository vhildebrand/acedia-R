% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpu_add.R
\name{gpu_add}
\alias{gpu_add}
\title{GPU-accelerated Vector Addition}
\usage{
gpu_add(a, b, force_cpu = FALSE, warn_fallback = TRUE)
}
\arguments{
\item{a}{A numeric vector}

\item{b}{A numeric vector of the same length as \code{a}}

\item{force_cpu}{Logical. If TRUE, forces CPU implementation (for testing/fallback)}

\item{warn_fallback}{Logical. If TRUE, warns when falling back to CPU implementation}
}
\value{
A numeric vector containing the element-wise sum of \code{a} and \code{b}
}
\description{
Performs element-wise addition of two numeric vectors using CUDA on the GPU.
This function provides a GPU-accelerated alternative to the standard R `+` operator
for large vectors where the computational overhead of GPU memory transfer is 
justified by the parallel processing benefits.
}
\details{
This function transfers the input vectors to GPU memory, performs the addition
using a CUDA kernel with parallel threads, and transfers the result back to CPU.
For small vectors (< 10^4 elements), the CPU version may be faster due to 
memory transfer overhead.

If GPU is not available or GPU operations fail, the function automatically
falls back to CPU computation with an optional warning.

For chained GPU operations or advanced workflows, consider using \code{gpu_tensor()}
to create tensor objects: 
\code{tensor_a + tensor_b} where \code{tensor_a} and \code{tensor_b} are gpuTensor objects.

See \code{\link{gpu_multiply}}, \code{\link{gpu_dot}} for related operations.
}
\examples{
\dontrun{
# Basic usage
a <- c(1, 2, 3, 4, 5)
b <- c(2, 3, 4, 5, 6)
result <- gpu_add(a, b)
print(result)  # [1] 3 5 7 9 11

# Verify against CPU
all.equal(result, a + b)  # Should be TRUE

# For chained operations, use gpuTensor objects:
tensor_a <- gpu_tensor(a, length(a))
tensor_b <- gpu_tensor(b, length(b))
tensor_result <- tensor_a + tensor_b  # Stays on GPU
result2 <- as.vector(tensor_result)   # Transfer back when needed
}

}
