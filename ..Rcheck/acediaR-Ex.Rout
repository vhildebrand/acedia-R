
R version 4.3.3 (2024-02-29) -- "Angel Food Cake"
Copyright (C) 2024 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "acediaR"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('acediaR')

Attaching package: ‘acediaR’

The following objects are masked from ‘package:stats’:

    reshape, var

> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("benchmark_gpu_multiply")
> ### * benchmark_gpu_multiply
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: benchmark_gpu_multiply
> ### Title: Benchmark GPU vs CPU Performance for Multiplication Operations
> ### Aliases: benchmark_gpu_multiply
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Basic benchmark
> ##D results <- benchmark_gpu_multiply()
> ##D print(results)
> ##D 
> ##D # Custom benchmark with specific sizes
> ##D results <- benchmark_gpu_multiply(
> ##D   sizes = c(1e4, 1e5, 1e6),
> ##D   iterations = 5
> ##D )
> ##D 
> ##D # Plot results
> ##D library(ggplot2)
> ##D ggplot(results, aes(x = size, y = speedup, color = operation)) +
> ##D   geom_line() +
> ##D   scale_x_log10() +
> ##D   labs(title = "GPU Speedup vs Vector Size",
> ##D        x = "Vector Size", y = "Speedup Ratio")
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("gpu_add")
> ### * gpu_add
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gpu_add
> ### Title: GPU-accelerated Vector Addition
> ### Aliases: gpu_add
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Add two large vectors on GPU (with automatic fallback)
> ##D n <- 1e6
> ##D a <- runif(n)
> ##D b <- runif(n)
> ##D result <- gpu_add(a, b)
> ##D 
> ##D # Force CPU implementation for testing
> ##D result_cpu <- gpu_add(a, b, force_cpu = TRUE)
> ##D 
> ##D # For chained operations, use gpuTensor objects:
> ##D tensor_a <- gpu_tensor(a, length(a))
> ##D tensor_b <- gpu_tensor(b, length(b)) 
> ##D tensor_result <- tensor_a + tensor_b  # Stays on GPU
> ##D result2 <- as.vector(tensor_result)  # Transfer back when needed
> ##D 
> ##D # Verify correctness against CPU
> ##D all.equal(result, a + b)
> ##D all.equal(result2, a + b)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("gpu_dot")
> ### * gpu_dot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gpu_dot
> ### Title: GPU-accelerated Dot Product
> ### Aliases: gpu_dot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Compute dot product of large vectors on GPU
> ##D n <- 1e6
> ##D a <- runif(n)
> ##D b <- runif(n)
> ##D result <- gpu_dot(a, b)
> ##D 
> ##D # Verify correctness against CPU
> ##D all.equal(result, sum(a * b))
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("gpu_multiply")
> ### * gpu_multiply
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gpu_multiply
> ### Title: GPU-accelerated Element-wise Vector Multiplication
> ### Aliases: gpu_multiply
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Multiply two large vectors on GPU (with automatic fallback)
> ##D n <- 1e6
> ##D a <- runif(n)
> ##D b <- runif(n)
> ##D result <- gpu_multiply(a, b)
> ##D 
> ##D # Force CPU implementation for testing
> ##D result_cpu <- gpu_multiply(a, b, force_cpu = TRUE)
> ##D 
> ##D # For chained operations, use gpuTensor objects:
> ##D tensor_a <- gpu_tensor(a, length(a))
> ##D tensor_b <- gpu_tensor(b, length(b)) 
> ##D tensor_result <- tensor_a * tensor_b  # Stays on GPU
> ##D result2 <- as.vector(tensor_result)  # Transfer back when needed
> ##D 
> ##D # Verify correctness against CPU
> ##D all.equal(result, a * b)
> ##D all.equal(result2, a * b)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("gpu_scale")
> ### * gpu_scale
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gpu_scale
> ### Title: GPU-accelerated Scalar Multiplication
> ### Aliases: gpu_scale
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Scale a large vector on GPU
> ##D n <- 1e6
> ##D x <- runif(n)
> ##D scalar <- 3.14
> ##D result <- gpu_scale(x, scalar)
> ##D 
> ##D # Verify correctness against CPU
> ##D all.equal(result, x * scalar)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("gpu_status")
> ### * gpu_status
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gpu_status
> ### Title: Check GPU Availability
> ### Aliases: gpu_status
> 
> ### ** Examples
> 
> ## Not run: 
> ##D if (gpu_available()) {
> ##D   cat("GPU acceleration enabled\n")
> ##D } else {
> ##D   cat("Using CPU-only mode\n")
> ##D }
> ## End(Not run)
> ## Not run: 
> ##D cat(gpu_info())
> ## End(Not run)
> ## Not run: 
> ##D mem_gb <- gpu_memory_available() / 1e9
> ##D cat("Available GPU memory:", round(mem_gb, 1), "GB\n")
> ## End(Not run)
> ## Not run: 
> ##D status <- gpu_status()
> ##D print(status)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("gpu_tensor")
> ### * gpu_tensor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gpu_tensor
> ### Title: Create a GPU Tensor
> ### Aliases: gpu_tensor
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Create a 2D tensor (matrix)
> ##D data <- 1:12
> ##D tensor <- gpu_tensor(data, shape = c(3, 4))
> ##D 
> ##D # Create a 3D tensor
> ##D tensor_3d <- gpu_tensor(runif(24), shape = c(2, 3, 4))
> ##D 
> ##D # Create with gradient tracking
> ##D tensor_grad <- gpu_tensor(1:6, shape = c(2, 3), requires_grad = TRUE)
> ## End(Not run)
> 
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  0.142 0.026 0.168 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
