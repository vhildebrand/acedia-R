// tensor_wrappers.cu
// C-style wrapper functions for R interface

#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <cuda_bf16.h>
#include <stdio.h>
#include <vector>
#include <limits>
#include "kernel_utils.cuh"
#include "../gpuTensor.h"
#include "../cuda_utils.h"

// Simple error checking function
inline void cudaSafeCall(cudaError_t err, const char* msg) {
    if (err != cudaSuccess) {
        char error_msg[256];
        snprintf(error_msg, sizeof(error_msg), "%s: %s", msg, cudaGetErrorString(err));
        throw std::runtime_error(error_msg);
    }
}

// Strided kernels (defined outside extern "C" to allow templates)
template<typename T, typename Op>
__global__ void strided_unary_kernel(
    cuda_utils::TensorDescriptor out_desc,
    cuda_utils::TensorDescriptor in_desc,
    size_t total_elements
) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= total_elements) return;
    
    // Convert linear index to coordinates
    int coords[8];
    out_desc.linear_to_coords(idx, coords);
    
    // Compute offsets using strides
    size_t in_offset = in_desc.compute_offset(coords);
    size_t out_offset = out_desc.compute_offset(coords);
    
    // Perform operation
    T* out_ptr = static_cast<T*>(out_desc.data);
    const T* in_ptr = static_cast<const T*>(in_desc.data);
    
    out_ptr[out_offset] = Op{}(in_ptr[in_offset]);
}

template<typename T, typename Op>
__global__ void strided_binary_kernel(
    cuda_utils::TensorDescriptor out_desc,
    cuda_utils::TensorDescriptor a_desc,
    cuda_utils::TensorDescriptor b_desc,
    size_t total_elements
) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= total_elements) return;
    
    // Convert linear index to coordinates
    int coords[8];
    out_desc.linear_to_coords(idx, coords);
    
    // Compute offsets using strides
    size_t a_offset = a_desc.compute_offset(coords);
    size_t b_offset = b_desc.compute_offset(coords);
    size_t out_offset = out_desc.compute_offset(coords);
    
    // Perform operation
    T* out_ptr = static_cast<T*>(out_desc.data);
    const T* a_ptr = static_cast<const T*>(a_desc.data);
    const T* b_ptr = static_cast<const T*>(b_desc.data);
    
    out_ptr[out_offset] = Op{}(a_ptr[a_offset], b_ptr[b_offset]);
}

// C-style wrapper functions for each type (for R interface)
extern "C" {

// Broadcast addition wrappers
void tensor_add_broadcast_float32(
    float* result, const float* a, const float* b,
    const int* a_strides, const int* b_strides, const int* result_strides,
    const int* shape, int ndims, size_t total_elements
) {
    launch_broadcast_binary(result, a, b, a_strides, b_strides, result_strides, shape, ndims, total_elements, AddOp{});
    cudaDeviceSynchronize();
}

void tensor_add_broadcast_float64(
    double* result, const double* a, const double* b,
    const int* a_strides, const int* b_strides, const int* result_strides,
    const int* shape, int ndims, size_t total_elements
) {
    launch_broadcast_binary(result, a, b, a_strides, b_strides, result_strides, shape, ndims, total_elements, AddOp{});
    cudaDeviceSynchronize();
}

// Broadcast multiplication wrappers
void tensor_mul_broadcast_float32(
    float* result, const float* a, const float* b,
    const int* a_strides, const int* b_strides, const int* result_strides,
    const int* shape, int ndims, size_t total_elements
) {
    launch_broadcast_binary(result, a, b, a_strides, b_strides, result_strides, shape, ndims, total_elements, MulOp{});
    cudaDeviceSynchronize();
}

void tensor_mul_broadcast_float64(
    double* result, const double* a, const double* b,
    const int* a_strides, const int* b_strides, const int* result_strides,
    const int* shape, int ndims, size_t total_elements
) {
    launch_broadcast_binary(result, a, b, a_strides, b_strides, result_strides, shape, ndims, total_elements, MulOp{});
    cudaDeviceSynchronize();
}

// Fill operations
void tensor_fill_float16(half* data, float value, size_t n) {
    launch_fill(data, __float2half(value), n);
    cudaDeviceSynchronize();
}

void tensor_fill_float32(float* data, float value, size_t n) {
    launch_fill(data, value, n);
    cudaDeviceSynchronize();
}

void tensor_fill_float64(double* data, double value, size_t n) {
    launch_fill(data, value, n);
    cudaDeviceSynchronize();
}

void tensor_fill_int8(int8_t* data, int value, size_t n) {
    launch_fill(data, static_cast<int8_t>(value), n);
    cudaDeviceSynchronize();
}

void tensor_fill_int32(int32_t* data, int value, size_t n) {
    launch_fill(data, value, n);
    cudaDeviceSynchronize();
}

void tensor_fill_int64(int64_t* data, long long value, size_t n) {
    launch_fill(data, static_cast<int64_t>(value), n);
    cudaDeviceSynchronize();
}

// Addition operations
void tensor_add_float16(half* result, const half* a, const half* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, AddOp{});
    cudaDeviceSynchronize();
}

void tensor_add_float32(float* result, const float* a, const float* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, AddOp{});
    cudaDeviceSynchronize();
}

void tensor_add_float64(double* result, const double* a, const double* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, AddOp{});
    cudaDeviceSynchronize();
}

void tensor_add_int8(int8_t* result, const int8_t* a, const int8_t* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, AddOp{});
    cudaDeviceSynchronize();
}

void tensor_add_int32(int32_t* result, const int32_t* a, const int32_t* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, AddOp{});
    cudaDeviceSynchronize();
}

void tensor_add_int64(int64_t* result, const int64_t* a, const int64_t* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, AddOp{});
    cudaDeviceSynchronize();
}

// Multiplication operations
void tensor_mul_float16(half* result, const half* a, const half* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, MulOp{});
    cudaDeviceSynchronize();
}

void tensor_mul_float32(float* result, const float* a, const float* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, MulOp{});
    cudaDeviceSynchronize();
}

void tensor_mul_float64(double* result, const double* a, const double* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, MulOp{});
    cudaDeviceSynchronize();
}

// Subtraction operations
void tensor_sub_float16(half* result, const half* a, const half* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, SubOp{});
    cudaDeviceSynchronize();
}

void tensor_sub_float32(float* result, const float* a, const float* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, SubOp{});
    cudaDeviceSynchronize();
}

void tensor_sub_float64(double* result, const double* a, const double* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, SubOp{});
    cudaDeviceSynchronize();
}

// Division operations
void tensor_div_float16(half* result, const half* a, const half* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, DivOp{});
    cudaDeviceSynchronize();
}

void tensor_div_float32(float* result, const float* a, const float* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, DivOp{});
    cudaDeviceSynchronize();
}

void tensor_div_float64(double* result, const double* a, const double* b, size_t n) {
    launch_elementwise_binary(result, a, b, n, DivOp{});
    cudaDeviceSynchronize();
}

// Scalar multiplication
void tensor_scalar_mul_float16(half* result, const half* input, float scalar, size_t n) {
    launch_elementwise_scalar(result, input, scalar, n, MulOp{});
    cudaDeviceSynchronize();
}

void tensor_scalar_mul_float32(float* result, const float* input, float scalar, size_t n) {
    launch_elementwise_scalar(result, input, scalar, n, MulOp{});
    cudaDeviceSynchronize();
}

void tensor_scalar_mul_float64(double* result, const double* input, double scalar, size_t n) {
    launch_elementwise_scalar(result, input, scalar, n, MulOp{});
    cudaDeviceSynchronize();
}

// Scalar addition
void tensor_scalar_add_float16(half* result, const half* input, float scalar, size_t n) {
    launch_elementwise_scalar(result, input, scalar, n, AddOp{});
    cudaDeviceSynchronize();
}

void tensor_scalar_add_float32(float* result, const float* input, float scalar, size_t n) {
    launch_elementwise_scalar(result, input, scalar, n, AddOp{});
    cudaDeviceSynchronize();
}

void tensor_scalar_add_float64(double* result, const double* input, double scalar, size_t n) {
    launch_elementwise_scalar(result, input, scalar, n, AddOp{});
    cudaDeviceSynchronize();
}

// Matrix multiplication
void tensor_matmul_float16(half* C, const half* A, const half* B, size_t M, size_t N, size_t K) {
    launch_matmul(C, A, B, M, N, K);
    cudaDeviceSynchronize();
}

void tensor_matmul_float32(float* C, const float* A, const float* B, size_t M, size_t N, size_t K) {
    launch_matmul(C, A, B, M, N, K);
    cudaDeviceSynchronize();
}

void tensor_matmul_float64(double* C, const double* A, const double* B, size_t M, size_t N, size_t K) {
    launch_matmul(C, A, B, M, N, K);
    cudaDeviceSynchronize();
}

// Sum reductions - moved to tensor_ops.cu to avoid duplicates

// Advanced tensor operations - moved to tensor_ops.cu to avoid duplicates

// Type conversion functions
void convert_float32_to_float16(half* output, const float* input, size_t n) {
    launch_type_conversion(output, input, n);
    cudaDeviceSynchronize();
}

void convert_float16_to_float32(float* output, const half* input, size_t n) {
    launch_type_conversion(output, input, n);
    cudaDeviceSynchronize();
}

void convert_float64_to_float32(float* output, const double* input, size_t n) {
    launch_type_conversion(output, input, n);
    cudaDeviceSynchronize();
}

void convert_float32_to_float64(double* output, const float* input, size_t n) {
    launch_type_conversion(output, input, n);
    cudaDeviceSynchronize();
}

void convert_int32_to_float32(float* output, const int32_t* input, size_t n) {
    launch_type_conversion(output, input, n);
    cudaDeviceSynchronize();
}

void convert_float32_to_int32(int32_t* output, const float* input, size_t n) {
    launch_type_conversion(output, input, n);
    cudaDeviceSynchronize();
}

// Mixed precision training utilities
void convert_gradients_fp32_to_fp16(half* fp16_grads, const float* fp32_grads, size_t n) {
    convert_float32_to_float16(fp16_grads, fp32_grads, n);
}

void accumulate_gradients_fp16_to_fp32(float* fp32_grads, const half* fp16_grads, size_t n) {
    // Convert to fp32 and accumulate
    float* temp_fp32;
    cudaMalloc(&temp_fp32, n * sizeof(float));
    convert_float16_to_float32(temp_fp32, fp16_grads, n);
    launch_elementwise_binary(fp32_grads, fp32_grads, temp_fp32, n, AddOp{});
    cudaFree(temp_fp32);
    cudaDeviceSynchronize();
}

// Strided copy operations
void tensor_strided_copy_float32(float* dest, const float* src, const int* strides, const int* shape, int ndims, size_t total_elements) {
    std::vector<int> stride_vec(strides, strides + ndims);
    std::vector<int> shape_vec(shape, shape + ndims);
    launch_strided_copy(dest, src, stride_vec, shape_vec, total_elements);
    cudaDeviceSynchronize();
}

void tensor_strided_copy_float64(double* dest, const double* src, const int* strides, const int* shape, int ndims, size_t total_elements) {
    std::vector<int> stride_vec(strides, strides + ndims);
    std::vector<int> shape_vec(shape, shape + ndims);
    launch_strided_copy(dest, src, stride_vec, shape_vec, total_elements);
    cudaDeviceSynchronize();
}

// Unary Math Operations and Reduction operations - moved to tensor_ops.cu to avoid duplicates



// Strided unary operation wrappers
void tensor_exp_strided_float32(const cuda_utils::TensorDescriptor& out_desc, 
                                const cuda_utils::TensorDescriptor& in_desc) {
    size_t total_elements = out_desc.total_size;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    strided_unary_kernel<float, ExpOp><<<grid_size, block_size>>>(
        out_desc, in_desc, total_elements
    );
    cudaDeviceSynchronize();
}

void tensor_exp_strided_float64(const cuda_utils::TensorDescriptor& out_desc, 
                                const cuda_utils::TensorDescriptor& in_desc) {
    size_t total_elements = out_desc.total_size;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    strided_unary_kernel<double, ExpOp><<<grid_size, block_size>>>(
        out_desc, in_desc, total_elements
    );
    cudaDeviceSynchronize();
}

void tensor_log_strided_float32(const cuda_utils::TensorDescriptor& out_desc, 
                                const cuda_utils::TensorDescriptor& in_desc) {
    size_t total_elements = out_desc.total_size;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    strided_unary_kernel<float, LogOp><<<grid_size, block_size>>>(
        out_desc, in_desc, total_elements
    );
    cudaDeviceSynchronize();
}

void tensor_log_strided_float64(const cuda_utils::TensorDescriptor& out_desc, 
                                const cuda_utils::TensorDescriptor& in_desc) {
    size_t total_elements = out_desc.total_size;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    strided_unary_kernel<double, LogOp><<<grid_size, block_size>>>(
        out_desc, in_desc, total_elements
    );
    cudaDeviceSynchronize();
}

void tensor_sqrt_strided_float32(const cuda_utils::TensorDescriptor& out_desc, 
                                 const cuda_utils::TensorDescriptor& in_desc) {
    size_t total_elements = out_desc.total_size;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    strided_unary_kernel<float, SqrtOp><<<grid_size, block_size>>>(
        out_desc, in_desc, total_elements
    );
    cudaDeviceSynchronize();
}

void tensor_sqrt_strided_float64(const cuda_utils::TensorDescriptor& out_desc, 
                                 const cuda_utils::TensorDescriptor& in_desc) {
    size_t total_elements = out_desc.total_size;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    strided_unary_kernel<double, SqrtOp><<<grid_size, block_size>>>(
        out_desc, in_desc, total_elements
    );
    cudaDeviceSynchronize();
}

// Strided binary operation wrappers
void tensor_add_strided_float32(const cuda_utils::TensorDescriptor& out_desc,
                                const cuda_utils::TensorDescriptor& a_desc,
                                const cuda_utils::TensorDescriptor& b_desc) {
    size_t total_elements = out_desc.total_size;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    strided_binary_kernel<float, AddOp><<<grid_size, block_size>>>(
        out_desc, a_desc, b_desc, total_elements
    );
    cudaDeviceSynchronize();
}

void tensor_add_strided_float64(const cuda_utils::TensorDescriptor& out_desc,
                                const cuda_utils::TensorDescriptor& a_desc,
                                const cuda_utils::TensorDescriptor& b_desc) {
    size_t total_elements = out_desc.total_size;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    strided_binary_kernel<double, AddOp><<<grid_size, block_size>>>(
        out_desc, a_desc, b_desc, total_elements
    );
    cudaDeviceSynchronize();
}

void tensor_mul_strided_float32(const cuda_utils::TensorDescriptor& out_desc,
                                const cuda_utils::TensorDescriptor& a_desc,
                                const cuda_utils::TensorDescriptor& b_desc) {
    size_t total_elements = out_desc.total_size;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    strided_binary_kernel<float, MulOp><<<grid_size, block_size>>>(
        out_desc, a_desc, b_desc, total_elements
    );
    cudaDeviceSynchronize();
}

void tensor_mul_strided_float64(const cuda_utils::TensorDescriptor& out_desc,
                                const cuda_utils::TensorDescriptor& a_desc,
                                const cuda_utils::TensorDescriptor& b_desc) {
    size_t total_elements = out_desc.total_size;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;
    
    strided_binary_kernel<double, MulOp><<<grid_size, block_size>>>(
        out_desc, a_desc, b_desc, total_elements
    );
    cudaDeviceSynchronize();
}

} // extern "C" 